{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark @ DESC -- Part I: the basics\n",
    "\n",
    "Author: **Julien Peloton** [@JulienPeloton](https://github.com/LSSTDESC/desc-spark/issues/new?body=@JulienPeloton)  \n",
    "Last Verifed to Run: **2018-10-23**  \n",
    "\n",
    "Welcome to the series of notebooks on Apache Spark! The main goal of this series is to get familiar with Apache Spark, and in particular its Python API called pyspark. \n",
    "\n",
    "__Learning objectives__\n",
    "\n",
    "- Apache Spark: what it is?\n",
    "- Installation @ HOME\n",
    "- Installation @ NERSC\n",
    "- Using the pyspark shell\n",
    "- Your first Spark program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark \n",
    "\n",
    "Apache Spark is a cluster computing framework, that is a set of tools to perform computation on a network of many machines. Spark started in 2009 as a research project, and it had a huge success so far in the industry. It is based on the so-called MapReduce cluster computing paradigm, popularized by the Hadoop framework using implicit data parallelism and fault tolerance. \n",
    "\n",
    "The core of Spark is written in Scala which is a general-purpose programming language that has been started in 2004 by Martin Odersky (EPFL). The language is inter-operable with Java and Java-like languages, and Scala executables run on the Java Virtual Machine (JVM). Note that Scala is not a pure functional programming language. It is multi-paradigm, including functional programming, imperative programming, object-oriented programming and concurrent computing.\n",
    "\n",
    "Spark provides many functionalities exposed through Scala/Python/Java/R API (Scala being the most complete one). As far as DESC is concerned, I would advocate to use the Python API (called pyspark) for obvious reasons. But feel free to put your hands on Scala, it's worth it. For those interested, you can have a look at this [tutorial](https://gitlab.in2p3.fr/MaitresNageurs/QuatreNages/Scala) on Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation @ HOME\n",
    "\n",
    "You might want to install Apache Spark on your laptop, to prototype programs and perform local checks. The easiest way to do so is to [download](https://spark.apache.org/downloads.html) a pre-built version of Spark (take the latest one). Untar it, move it to the location you want, and update your path such that it can be found when you launch a job:\n",
    "\n",
    "```bash\n",
    "# Put those lines in your HOME/.bash_profile\n",
    "SPARK_HOME=/path/to/spark\n",
    "export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH\n",
    "```\n",
    "\n",
    "Latest version of Spark should run on Java 8+, but I recommend using it on Java 8. On macOS, to see the different java jdk installed on your machine: \n",
    "\n",
    "```\n",
    "/usr/libexec/java_home -V\n",
    "```\n",
    "\n",
    "If Java 8 is not present, download the JDK and set it using:\n",
    "\n",
    "```bash\n",
    "# Put this line in your HOME/.bash_profile, with the \n",
    "# version number you just downloaded.\n",
    "export JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_151`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation @ NERSC\n",
    "\n",
    "batch mode + JupyterLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
