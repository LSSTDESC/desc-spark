{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark @ DESC -- Part I: Installation and first steps\n",
    "\n",
    "Author: **Julien Peloton** [@JulienPeloton](https://github.com/LSSTDESC/desc-spark/issues/new?body=@JulienPeloton)  \n",
    "Last Verifed to Run: **2018-10-23**  \n",
    "\n",
    "Welcome to the series of notebooks on Apache Spark! The main goal of this series is to get familiar with Apache Spark, and in particular its Python API called pyspark. \n",
    "\n",
    "__Learning objectives__\n",
    "\n",
    "- Apache Spark: what it is?\n",
    "- Installation @ HOME\n",
    "- Installation @ NERSC\n",
    "- Using the pyspark shell\n",
    "- Your first Spark program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark \n",
    "\n",
    "Apache Spark is a cluster computing framework, that is a set of tools to perform computation on a network of many machines. Spark started in 2009 as a research project, and it had a huge success so far in the industry. It is based on the so-called MapReduce cluster computing paradigm, popularized by the Hadoop framework using implicit data parallelism and fault tolerance. \n",
    "\n",
    "The core of Spark is written in Scala which is a general-purpose programming language that has been started in 2004 by Martin Odersky (EPFL). The language is inter-operable with Java and Java-like languages, and Scala executables run on the Java Virtual Machine (JVM). Note that Scala is not a pure functional programming language. It is multi-paradigm, including functional programming, imperative programming, object-oriented programming and concurrent computing.\n",
    "\n",
    "Spark provides many functionalities exposed through Scala/Python/Java/R API (Scala being the most complete one). As far as DESC is concerned, I would advocate to use the Python API (called pyspark) for obvious reasons. But feel free to put your hands on Scala, it's worth it. For those interested, you can have a look at this [tutorial](https://gitlab.in2p3.fr/MaitresNageurs/QuatreNages/Scala) on Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation @ HOME\n",
    "\n",
    "You might want to install Apache Spark on your laptop, to prototype programs and perform local checks. The easiest way to do so is to [download](https://spark.apache.org/downloads.html) a pre-built version of Spark (take the latest one). Untar it, move it to the location you want, and update your path such that it can be found when you launch a job:\n",
    "\n",
    "```bash\n",
    "# Put those lines in your HOME/.bash_profile\n",
    "SPARK_HOME=/path/to/spark\n",
    "export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH\n",
    "```\n",
    "\n",
    "Latest version of Spark should run on Java 8+, but I recommend using it on Java 8. On macOS, to see the different java jdk installed on your machine: \n",
    "\n",
    "```\n",
    "/usr/libexec/java_home -V\n",
    "```\n",
    "\n",
    "If Java 8 is not present, download the JDK and set it using:\n",
    "\n",
    "```bash\n",
    "# Put this line in your HOME/.bash_profile, with the \n",
    "# version number you just downloaded. Example:\n",
    "export JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_151`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation @ NERSC\n",
    "\n",
    "### batch mode \n",
    "\n",
    "NERSC provides support to run Spark at scale. Note that for Spark version 2.3.0+, Spark runs inside of [Shifter](http://www.nersc.gov/research-and-development/user-defined-images/). Complete information is available at [spark-distributed-analytic-framework](www.nersc.gov/users/data-analytics/data-analytics-2/spark-distributed-analytic-framework/).\n",
    "\n",
    "### JupyterLab\n",
    "\n",
    "We provide kernels to work with Apache Spark and DESC. To get a DESC python + Apache Spark kernel, follow these steps:\n",
    "\n",
    "```bash\n",
    "# Clone the repo\n",
    "git clone https://github.com/astrolabsoftware/spark-kernel-nersc.git\n",
    "cd spark-kernel-nersc\n",
    "\n",
    "# Where the Spark logs will be stored\n",
    "# Logs can be then be browsed from the Spark UI\n",
    "LOGDIR=${SCRATCH}/spark/event_logs\n",
    "mkdir -p ${LOGDIR}\n",
    "\n",
    "# Resource to use. Here we will use 4 threads.\n",
    "RESOURCE=local[4]\n",
    "\n",
    "# Extra libraries (comma separated if many) to use.\n",
    "SPARKFITS=com.github.astrolabsoftware:spark-fits_2.11:0.7.1\n",
    "\n",
    "# Create the kernel - it will be stored under\n",
    "# $HOME/.ipython/kernels/<kernelname>\n",
    "python makekernel.py \\\n",
    "  -kernelname desc-pyspark --desc \\\n",
    "  -pyspark_args \"--master ${RESOURCE} \\\n",
    "  --conf spark.eventLog.enabled=true \\\n",
    "  --conf spark.eventLog.dir=file://${LOGDIR} \\\n",
    "  --conf spark.history.fs.logDirectory=file://${LOGDIR} \\\n",
    "  --packages ${SPARKFITS}\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "And then select the kernel `desc-pyspark` in the JupyerLab [interface](https://jupyter-dev.nersc.gov/).\n",
    "More information can be found at [spark-kernel-nersc](https://github.com/astrolabsoftware/spark-kernel-nersc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the pyspark shell (@ HOME or NERSC interactive)\n",
    "\n",
    "### Python/IPython shells\n",
    "\n",
    "To access the pyspark shell, just type `pyspark` in a terminal. You will be redirected to the standard python shell, augmented with spark environment and pre-loaded objects such as the `sparkContext` (`sc`) and the `sparkSession` (`spark`). Between you and me, the standard python shell is rather ugly and lacks of nice functionalities. If you really want to increase your productivity, you probably want to switch to IPython. Just type in your shell:\n",
    "\n",
    "```\n",
    "PYSPARK_DRIVER_PYTHON=ipython pyspark\n",
    "```\n",
    "And you should see (with your corresponding Spark, Python and IPython versions):\n",
    "\n",
    "```\n",
    "Python 3.7.0 (default, Jun 28 2018, 07:39:16)\n",
    "Type 'copyright', 'credits' or 'license' for more information\n",
    "IPython 7.0.1 -- An enhanced Interactive Python. Type '?' for help.\n",
    "2018-10-24 21:13:45 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.7.0 (default, Jun 28 2018 07:39:16)\n",
    "SparkSession available as 'spark'.\n",
    "\n",
    "In [1]:\n",
    "```\n",
    "\n",
    "If it complains about IPython not found but you know it is installed somewhere, just specify the whole path to it (to see it: `which ipython`). As said previously, you'll have your Spark environment loaded and few objects ready:\n",
    "\n",
    "```python\n",
    "In [1]: # Spark Session \n",
    "In [2]: spark\n",
    "Out[2]: <pyspark.sql.session.SparkSession at 0x10d8d6e80>\n",
    "    \n",
    "In [3]: # Spark Context \n",
    "In [4]: sc\n",
    "Out[4]: <SparkContext master=local[*] appName=PySparkShell>\n",
    "```\n",
    "\n",
    "### Specifying resources\n",
    "\n",
    "By default ... see above ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first pro\n",
    "\n",
    "```\n",
    "spark-submit ....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
